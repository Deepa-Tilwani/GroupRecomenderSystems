{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GroupRec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h4>Group Class :</h4>\n",
    "The class 'Group' is responsible for generating random groups of different sizes : small, medium and large and performing evaluations of different methods AF, BF and WBF used for recommendation for these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import configparser\n",
    "\n",
    "class Group:\n",
    "    def __init__(self, members, candidate_items, ratings):\n",
    "        # member ids\n",
    "        self.members = sorted(members)\n",
    "        \n",
    "        # List of items that can be recommended.\n",
    "        # These should not have been watched by any member of group.\n",
    "        self.candidate_items = candidate_items\n",
    "\n",
    "        self.actual_recos = []\n",
    "        self.false_positive = []\n",
    "        \n",
    "        self.ratings_per_member = [np.size(ratings[member].nonzero()) for member in self.members]\n",
    "        \n",
    "        # AF\n",
    "        self.grp_factors_af = []\n",
    "        self.bias_af = 0\n",
    "        self.precision_af = 0\n",
    "        self.recall_af = 0\n",
    "        self.reco_list_af = [] \n",
    "        \n",
    "        # BF\n",
    "        self.grp_factors_bf = []\n",
    "        self.bias_bf = 0\n",
    "        self.precision_bf = 0\n",
    "        self.recall_bf = 0\n",
    "        self.reco_list_bf = []\n",
    "        \n",
    "        # WBF\n",
    "        self.grp_factors_wbf = []\n",
    "        self.bias_wbf = 0\n",
    "        self.precision_wbf = 0\n",
    "        self.recall_wbf = 0\n",
    "        self.weight_matrix_wbf = []\n",
    "        self.reco_list_wbf = []\n",
    "\n",
    "#Configuration reader.\n",
    "class Config:\n",
    "    def __init__(self, config_file_path):\n",
    "        self.config_file_path = config_file_path\n",
    "\n",
    "        configParser = configparser.RawConfigParser()\n",
    "        configParser.read(config_file_path)\n",
    "        \n",
    "        #movie lens 100k dataset, 80 - 20 train/test ratio, present in data directory\n",
    "        self.training_file = configParser.get('Config', 'training_file')\n",
    "        self.testing_file = configParser.get('Config', 'testing_file')\n",
    "        \n",
    "        self.small_grp_size = int(configParser.get('Config', 'small_grp_size'))\n",
    "        self.medium_grp_size = int(configParser.get('Config', 'medium_grp_size'))\n",
    "        self.large_grp_size = int(configParser.get('Config', 'large_grp_size'))\n",
    "        \n",
    "        self.max_iterations_mf = int(configParser.get('Config', 'max_iterations_mf'))\n",
    "        self.lambda_mf = float(configParser.get('Config', 'lambda_mf'))\n",
    "        self.learning_rate_mf = float(configParser.get('Config', 'learning_rate_mf'))\n",
    "        \n",
    "        self.num_factors = int(configParser.get('Config', 'num_factors'))\n",
    "        \n",
    "        #AF (after factorization)\n",
    "        self.rating_threshold_af = float(configParser.get('Config', 'rating_threshold_af'))\n",
    "        self.num_recos_af = int(configParser.get('Config', 'num_recos_af'))\n",
    "        \n",
    "        #BF (before factorization)\n",
    "        self.rating_threshold_bf = float(configParser.get('Config', 'rating_threshold_bf'))\n",
    "        self.num_recos_bf = int(configParser.get('Config', 'num_recos_bf'))\n",
    "        \n",
    "        #WBF (weighted before factorization)\n",
    "        self.rating_threshold_wbf = float(configParser.get('Config', 'rating_threshold_wbf'))\n",
    "        self.num_recos_wbf = int(configParser.get('Config', 'num_recos_wbf'))\n",
    "        \n",
    "        self.is_debug = configParser.getboolean('Config', 'is_debug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Creating list of movies which can be recommended :</h4>\n",
    "<p>The following function creates a list of movies which have not been watched by any of the members of the group.</p>\n",
    "Our recommendation will be made out of these movies only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def find_candidate_items(ratings, members):\n",
    "    if len(members) == 0: return []\n",
    "\n",
    "    unwatched_items = np.argwhere(ratings[members[0]] == 0)\n",
    "    for member in members:\n",
    "        cur_unwatched = np.argwhere(ratings[member] == 0)\n",
    "        unwatched_items = np.intersect1d(unwatched_items, cur_unwatched)\n",
    "\n",
    "    return unwatched_items\n",
    "Group.find_candidate_items = find_candidate_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def non_testable_items(members, ratings): \n",
    "    non_eval_items = np.argwhere(ratings[members[0]] == 0)\n",
    "    for member in members:\n",
    "        cur_non_eval_items = np.argwhere(ratings[member] == 0)\n",
    "        non_eval_items = np.intersect1d(non_eval_items, cur_non_eval_items)\n",
    "    return non_eval_items\n",
    "Group.non_testable_items = non_testable_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Generating groups! </h4>\n",
    "Now we will generate groups from the available users. For better evaluation of our recommendation apporaches, we have to make sure that there are enough items to test upon. So we have set the testable_threshold to be 50, which basically means that there are at least 50 movies in the test data set which have been rated by at least one member of the group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def generate_groups(cfg, ratings, test_ratings, num_users, count, size, disjoint = True):\n",
    "    avbl_users = [i for i in range(num_users)]\n",
    "    groups = []\n",
    "    testable_threshold = 50\n",
    "\n",
    "    iter_idx = 0\n",
    "    while iter_idx in range(count):\n",
    "        group_members = np.random.choice(avbl_users, size = size, replace = False)\n",
    "        candidate_items = Group.find_candidate_items(ratings, group_members)\n",
    "        non_eval_items = Group.non_testable_items(group_members, test_ratings)\n",
    "        testable_items = np.setdiff1d(candidate_items, non_eval_items)\n",
    "\n",
    "        if len(candidate_items) != 0 and len(testable_items) >= testable_threshold:\n",
    "            groups += [Group(group_members, candidate_items, ratings)]\n",
    "            avbl_users = np.setdiff1d(avbl_users, group_members)\n",
    "            iter_idx += 1\n",
    "\n",
    "    return groups\n",
    "    \n",
    "Group.generate_groups = generate_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Prediction!</h4>\n",
    "<p>Now that the groups have been formed, this is the method for finally predicting the movies!</p>\n",
    "We have kept the threshold for predicted rating for an item to be 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_actual_recommendations(self, ratings, threshold):\n",
    "    non_eval_items = Group.non_testable_items(self.members, ratings)\n",
    "\n",
    "    items = np.argwhere(np.logical_or(ratings[self.members[0]] >= threshold, ratings[self.members[0]] == 0)).flatten()\n",
    "    fp = np.argwhere(np.logical_and(ratings[self.members[0]] > 0, ratings[self.members[0]] < threshold)).flatten()\n",
    "    for member in self.members:\n",
    "        cur_items = np.argwhere(np.logical_or(ratings[member] >= threshold, ratings[member] == 0)).flatten()\n",
    "        fp = np.union1d(fp, np.argwhere(np.logical_and(ratings[member] > 0, ratings[member] < threshold)).flatten())\n",
    "        items = np.intersect1d(items, cur_items)\n",
    "\n",
    "    items = np.setdiff1d(items, non_eval_items)\n",
    "\n",
    "    self.actual_recos = items\n",
    "    self.false_positive = fp\n",
    "\n",
    "Group.generate_actual_recommendations  = generate_actual_recommendations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Evaluation :</h4>\n",
    "<p>The following three functions are used for the evaluation of the three methods AF, BF and WBF respectively.</p>\n",
    "We are evaluating the methods using their Precision and Recall for different sizes of groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_af(self, is_debug=False):\n",
    "    tp = float(np.intersect1d(self.actual_recos, self.reco_list_af).size)\n",
    "    fp = float(np.intersect1d(self.false_positive, self.reco_list_af).size)\n",
    "\n",
    "    try:\n",
    "        self.precision_af = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        self.precision_af = np.NaN\n",
    "\n",
    "    try:\n",
    "        self.recall_af = tp / self.actual_recos.size\n",
    "    except ZeroDivisionError:\n",
    "        self.recall_af = np.NaN\n",
    "\n",
    "    if is_debug:\n",
    "        print ('tp: ', tp)\n",
    "        print ('fp: ', fp)\n",
    "        print ('precision_af: ', self.precision_af)\n",
    "        print ('recall_af: ', self.recall_af)\n",
    "\n",
    "    return self.precision_af, self.recall_af, tp, fp\n",
    "Group.evaluate_af = evaluate_af"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bf(self, is_debug=False):\n",
    "    tp = float(np.intersect1d(self.actual_recos, self.reco_list_bf).size)\n",
    "    fp = float(np.intersect1d(self.false_positive, self.reco_list_bf).size)\n",
    "\n",
    "    try:\n",
    "        self.precision_bf = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        self.precision_bf = np.NaN\n",
    "\n",
    "    try:\n",
    "        self.recall_bf = tp / self.actual_recos.size\n",
    "    except ZeroDivisionError:\n",
    "        self.recall_bf = np.NaN\n",
    "\n",
    "    if is_debug:\n",
    "        print ('tp: ', tp)\n",
    "        print ('fp: ', fp)\n",
    "        print ('precision_bf: ', self.precision_bf)\n",
    "        print ('recall_bf: ', self.recall_bf)\n",
    "\n",
    "    return self.precision_bf, self.recall_bf, tp, fp\n",
    "Group.evaluate_bf = evaluate_bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_wbf(self, is_debug=False):\n",
    "    tp = float(np.intersect1d(self.actual_recos, self.reco_list_wbf).size)\n",
    "    fp = float(np.intersect1d(self.false_positive, self.reco_list_wbf).size)\n",
    "\n",
    "    try:\n",
    "        self.precision_wbf = tp / (tp + fp)\n",
    "    except ZeroDivisionError:\n",
    "        self.precision_wbf = np.NaN\n",
    "\n",
    "    try:\n",
    "        self.recall_wbf = tp / self.actual_recos.size\n",
    "    except ZeroDivisionError:\n",
    "        self.recall_wbf = np.NaN\n",
    "\n",
    "    if is_debug:\n",
    "        print ('tp: ', tp)\n",
    "        print ('fp: ', fp)\n",
    "        print ('precision_bf: ', self.precision_wbf)\n",
    "        print ('recall_bf: ', self.recall_wbf)\n",
    "\n",
    "    return self.precision_wbf, self.recall_wbf, tp, fp\n",
    "Group.evaluate_wbf = evaluate_wbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Aggregator Class :</h4>\n",
    "This class is responsible for defining different ways to aggregate factors for the member of the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "class Aggregators:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    #pass ratings or factors as input\n",
    "    @staticmethod\n",
    "    def average(arr):\n",
    "        return np.average(arr, axis = 0, weights = None)\n",
    "\n",
    "    @staticmethod\n",
    "    def average_bf(arr):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "            arr[arr == 0] = np.nan\n",
    "            return np.nanmean(arr, axis=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def weighted_average(arr, weights):\n",
    "        return np.average(arr, axis = 0, weights = weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>GroupRec Class :</h4>\n",
    "This is our main class responsible for reading the data, defining methods for our appoaches and finally evaluating them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as ps\n",
    "\n",
    "\n",
    "# overflow warnings should be raised as errors\n",
    "np.seterr(over='raise')\n",
    "\n",
    "class GroupRec:\n",
    "    def __init__(self):\n",
    "        self.cfg = Config(r\"config.conf\")\n",
    "        \n",
    "        # training and testing matrices\n",
    "        self.ratings = None\n",
    "        self.test_ratings = None\n",
    "\n",
    "        self.groups = []\n",
    "        \n",
    "        # read data into above matrices\n",
    "        self.read_data()\n",
    "        \n",
    "        self.num_users = self.ratings.shape[0]\n",
    "        self.num_items = self.ratings.shape[1]\n",
    "        \n",
    "        # predicted ratings matrix based on factors.\n",
    "        self.predictions = np.zeros((self.num_users, self.num_items))\n",
    "        \n",
    "        # output after svd factorization\n",
    "        # initialize all unknowns with random values from -1 to 1\n",
    "        self.user_factors = np.random.uniform(-1, 1, (self.ratings.shape[0], self.cfg.num_factors))\n",
    "        self.item_factors = np.random.uniform(-1, 1, (self.ratings.shape[1], self.cfg.num_factors))\n",
    "\n",
    "        self.user_biases = np.zeros(self.num_users)\n",
    "        self.item_biases = np.zeros(self.num_items)\n",
    "        \n",
    "        # global mean of ratings a.k.a mu\n",
    "        self.ratings_global_mean = 0\n",
    "\n",
    "    # add list of groups\n",
    "    def add_groups(self, groups):\n",
    "        self.groups = groups\n",
    "    \n",
    "    # remove groups\n",
    "    def remove_groups(self, groups):\n",
    "        self.groups = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Reading the data : </h4>\n",
    "We have used 'pandas' library for reading testing data and training data from the csv file.\n",
    "We will finally generate our user * item ratings matrix here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training and testing data into matrices\n",
    "def read_data(self):\n",
    "    column_headers = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "\n",
    "    print ('Reading training data from ', self.cfg.training_file, '...')\n",
    "    training_data = ps.read_csv(self.cfg.training_file, sep='\\t', names=column_headers)\n",
    "\n",
    "    print ('Reading testing data from ', self.cfg.testing_file, '...')\n",
    "    testing_data = ps.read_csv(self.cfg.testing_file, sep='\\t', names=column_headers)\n",
    "\n",
    "    num_users = max(training_data.user_id.unique())\n",
    "    num_items = max(training_data.item_id.unique())\n",
    "\n",
    "    self.ratings = np.zeros((num_users, num_items))\n",
    "    self.test_ratings = np.zeros((num_users, num_items))\n",
    "\n",
    "    for row in training_data.itertuples(index=False):\n",
    "        self.ratings[row.user_id - 1, row.item_id - 1] = row.rating\n",
    "\n",
    "    for row in testing_data.itertuples(index=False):\n",
    "        self.test_ratings[row.user_id - 1, row.item_id - 1] = row.rating\n",
    "        \n",
    "GroupRec.read_data = read_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Matrix Factorization : </h4>\n",
    "Now we would like to factorize the rating matrix. \n",
    "We have considered the number of factors to be 15.\n",
    "And we are using gradient descent for error minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_factorize(self):\n",
    "    #solve for these for matrix ratings        \n",
    "    ratings_row, ratings_col = self.ratings.nonzero()\n",
    "    num_ratings = len(ratings_row)\n",
    "    learning_rate = self.cfg.learning_rate_mf\n",
    "    regularization = self.cfg.lambda_mf\n",
    "\n",
    "    self.ratings_global_mean = np.mean(self.ratings[np.where(self.ratings != 0)])\n",
    "\n",
    "    print( 'Doing matrix factorization...')\n",
    "    try:\n",
    "        for iter in range(self.cfg.max_iterations_mf):\n",
    "            print ('Iteration: ', iter)\n",
    "            rating_indices = np.arange(num_ratings)\n",
    "            np.random.shuffle(rating_indices)\n",
    "\n",
    "            for idx in rating_indices:\n",
    "                user = ratings_row[idx]\n",
    "                item = ratings_col[idx]\n",
    "\n",
    "                pred = self.predict_user_rating(user, item)\n",
    "                error = self.ratings[user][item] - pred\n",
    "\n",
    "                self.user_factors[user] += learning_rate \\\n",
    "                                            * ((error * self.item_factors[item]) - (regularization * self.user_factors[user]))\n",
    "                self.item_factors[item] += learning_rate \\\n",
    "                                            * ((error * self.user_factors[user]) - (regularization * self.item_factors[item]))\n",
    "\n",
    "                self.user_biases[user] += learning_rate * (error - regularization * self.user_biases[user])\n",
    "                self.item_biases[item] += learning_rate * (error - regularization * self.item_biases[item])\n",
    "\n",
    "            self.sgd_mse()\n",
    "\n",
    "    except FloatingPointError:\n",
    "        print( 'Floating point Error: ')\n",
    "GroupRec.sgd_factorize = sgd_factorize\n",
    "\n",
    "\n",
    "def sgd_mse(self):\n",
    "    self.predict_all_ratings()\n",
    "    predicted_training_ratings = self.predictions[self.ratings.nonzero()].flatten()\n",
    "    actual_training_ratings = self.ratings[self.ratings.nonzero()].flatten()\n",
    "\n",
    "    predicted_test_ratings = self.predictions[self.test_ratings.nonzero()].flatten()\n",
    "    actual_test_ratings = self.test_ratings[self.test_ratings.nonzero()].flatten()\n",
    "\n",
    "    training_mse = mean_squared_error(predicted_training_ratings, actual_training_ratings)\n",
    "    print ('training mse: ', training_mse)\n",
    "    test_mse = mean_squared_error(predicted_test_ratings, actual_test_ratings)\n",
    "    print ('test mse: ', test_mse)\n",
    "GroupRec.sgd_mse = sgd_mse\n",
    "\n",
    "\n",
    "def predict_user_rating(self, user, item):\n",
    "    prediction = self.ratings_global_mean + self.user_biases[user] + self.item_biases[item]\n",
    "    prediction += self.user_factors[user, :].dot(self.item_factors[item, :].T)\n",
    "    return prediction\n",
    "GroupRec.predict_user_rating = predict_user_rating\n",
    "\n",
    "def predict_group_rating(self, group, item, method):\n",
    "    if (method == 'af'):\n",
    "        factors = group.grp_factors_af; bias_group = group.bias_af\n",
    "    elif (method == 'bf'):\n",
    "        factors = group.grp_factors_bf; bias_group = group.bias_bf\n",
    "    elif (method == 'wbf'):\n",
    "        factors = group.grp_factors_wbf; bias_group = group.bias_wbf\n",
    "\n",
    "    return self.ratings_global_mean + bias_group + self.item_biases[item] \\\n",
    "                                    + np.dot(factors.T, self.item_factors[item])\n",
    "GroupRec.predict_group_rating = predict_group_rating\n",
    "\n",
    "def predict_all_ratings(self):\n",
    "    for user in range(self.num_users):\n",
    "        for item in range(self.num_items):\n",
    "            self.predictions[user, item] = self.predict_user_rating(user, item)\n",
    "GroupRec.predict_all_ratings = predict_all_ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>After Factorization (AF) Method Definition.....</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AF method\n",
    "def af_runner(self, groups = None, aggregator = Aggregators.average):\n",
    "    #if groups is not passed, use self.groups\n",
    "    if (groups is None):\n",
    "        groups = self.groups\n",
    "\n",
    "    #calculate factors\n",
    "    for group in groups:\n",
    "        member_factors = self.user_factors[group.members, :]\n",
    "        member_biases = self.user_biases[group.members]\n",
    "\n",
    "        #aggregate the factors\n",
    "        if (aggregator == Aggregators.average):\n",
    "            group.grp_factors_af = aggregator(member_factors)\n",
    "            group.bias_af = aggregator(member_biases)\n",
    "        elif (aggregator == Aggregators.weighted_average):\n",
    "            group.grp_factors_af = aggregator(member_factors, weights = group.ratings_per_member)\n",
    "            group.bias_af = aggregator(member_biases, weights = group.ratings_per_member)\n",
    "\n",
    "        #predict ratings for all candidate items\n",
    "        group_candidate_ratings = {}\n",
    "        for idx, item in enumerate(group.candidate_items):\n",
    "            cur_rating = self.predict_group_rating(group, item, 'af')\n",
    "\n",
    "            if (cur_rating > self.cfg.rating_threshold_af):\n",
    "                group_candidate_ratings[item] = cur_rating\n",
    "\n",
    "        #sort and filter to keep top 'num_recos_af' recommendations\n",
    "        group_candidate_ratings = sorted(group_candidate_ratings.items(), key=lambda x: x[1], reverse=True)[:self.cfg.num_recos_af]\n",
    "\n",
    "        group.reco_list_af = np.array([rating_tuple[0] for rating_tuple in group_candidate_ratings])\n",
    "\n",
    "GroupRec.af_runner = af_runner\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Before Factorization(BF) Method.....</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " def bf_runner(self, groups=None, aggregator=Aggregators.average_bf):\n",
    "    # aggregate user ratings into virtual group\n",
    "    # calculate factors of group\n",
    "    lamb = self.cfg.lambda_mf\n",
    "\n",
    "    for group in groups:\n",
    "        all_movies = np.arange(len(self.ratings.T))\n",
    "        watched_items = sorted(list(set(all_movies) - set(group.candidate_items)))\n",
    "\n",
    "        group_rating = self.ratings[group.members, :]\n",
    "        agg_rating = aggregator(group_rating)\n",
    "        s_g = []\n",
    "        for j in watched_items:\n",
    "            s_g.append(agg_rating[j] - self.ratings_global_mean - self.item_biases[j])\n",
    "\n",
    "        # creating matrix A : contains rows of [item_factors of items in watched_list + '1' vector]\n",
    "        A = np.zeros((0, self.cfg.num_factors))\n",
    "\n",
    "        for item in watched_items:\n",
    "            A = np.vstack([A, self.item_factors[item]])\n",
    "        v = np.ones((len(watched_items), 1))\n",
    "        A = np.c_[A, v]\n",
    "\n",
    "        factor_n_bias = np.dot(np.linalg.inv(np.dot(A.T, A) + lamb * np.identity(self.cfg.num_factors + 1)), np.dot(A.T, s_g))\n",
    "        group.grp_factors_bf = factor_n_bias[:-1]\n",
    "        group.bias_bf = factor_n_bias[-1]\n",
    "\n",
    "        # Making recommendations on candidate list :\n",
    "        group_candidate_ratings = {}\n",
    "        for idx, item in enumerate(group.candidate_items):\n",
    "            cur_rating = self.predict_group_rating(group, item, 'bf')\n",
    "\n",
    "            if (cur_rating > self.cfg.rating_threshold_bf):\n",
    "                group_candidate_ratings[item] = cur_rating\n",
    "\n",
    "        # sort and filter to keep top 'num_recos_bf' recommendations\n",
    "        group_candidate_ratings = sorted(group_candidate_ratings.items(), key=lambda x: x[1], reverse=True)[\n",
    "                                  :self.cfg.num_recos_bf]\n",
    "\n",
    "        group.reco_list_bf = np.array([rating_tuple[0] for rating_tuple in group_candidate_ratings])\n",
    "        \n",
    "GroupRec.bf_runner = bf_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Weighted Before Factorization Method (WBF).....</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from statistics import stdev \n",
    "def wbf_runner(self, groups=None, aggregator=Aggregators.average_bf):\n",
    "    # aggregate user ratings into virtual group\n",
    "    # calculate factors of group\n",
    "    lamb = self.cfg.lambda_mf\n",
    "    for group in groups:\n",
    "        all_movies = np.arange(len(self.ratings.T))\n",
    "        watched_items = sorted(list(set(all_movies) - set(group.candidate_items)))\n",
    "\n",
    "        group_rating = self.ratings[group.members, :]\n",
    "        agg_rating = aggregator(group_rating)\n",
    "        s_g = []\n",
    "        for j in watched_items:\n",
    "            s_g.append(agg_rating[j] - self.ratings_global_mean - self.item_biases[j])\n",
    "\n",
    "        # creating matrix A : contains rows of [item_factors of items in watched_list + '1' vector]\n",
    "        A = np.zeros((0, self.cfg.num_factors))  # 3 is the number of features here = K\n",
    "\n",
    "        for item in watched_items:\n",
    "            A = np.vstack([A, self.item_factors[item]])\n",
    "        v = np.ones((len(watched_items), 1))\n",
    "        A = np.c_[A, v]\n",
    "\n",
    "        wt = []\n",
    "        for item in watched_items:\n",
    "            rated = np.argwhere(self.ratings[:, item] != 0)  # list of users who have rated this movie\n",
    "            watched = np.in1d(rated, group)  # list of group members who have watched this movie\n",
    "            std_dev = stdev( self.ratings[:, item])  # std deviation for the rating of the item\n",
    "            wt += [len(watched) / float(len(group.members)) * 1 / (1 + std_dev)]  # list containing diagonal elements\n",
    "        W = np.diag(wt)  # diagonal weight matrix\n",
    "\n",
    "        factor_n_bias = np.dot(np.linalg.inv(np.dot(np.dot(A.T, W),A) + lamb * np.identity(self.cfg.num_factors + 1)),\n",
    "                               np.dot(np.dot(A.T, W), s_g))\n",
    "        group.grp_factors_wbf = factor_n_bias[:-1]\n",
    "        group.bias_wbf = factor_n_bias[-1]\n",
    "\n",
    "        # Making recommendations on candidate list :\n",
    "        group_candidate_ratings = {}\n",
    "        for idx, item in enumerate(group.candidate_items):\n",
    "            cur_rating = self.predict_group_rating(group, item, 'wbf')\n",
    "\n",
    "            if (cur_rating > self.cfg.rating_threshold_wbf):\n",
    "                group_candidate_ratings[item] = cur_rating\n",
    "\n",
    "        # sort and filter to keep top 'num_recos_wbf' recommendations\n",
    "        group_candidate_ratings = sorted(group_candidate_ratings.items(), key=lambda x: x[1], reverse=True)[\n",
    "                                  :self.cfg.num_recos_wbf]\n",
    "\n",
    "        group.reco_list_wbf = np.array([rating_tuple[0] for rating_tuple in group_candidate_ratings])\n",
    "\n",
    "GroupRec.wbf_runner = wbf_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Evaluating our methods......</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(self):\n",
    "    # For AF\n",
    "    af_precision_list = []\n",
    "    af_recall_list = []\n",
    "    print (\"\\n#########-------For AF-------#########\")\n",
    "    for grp in self.groups:\n",
    "        grp.generate_actual_recommendations(self.test_ratings, self.cfg.rating_threshold_af)\n",
    "        (precision, recall, tp, fp) = grp.evaluate_af()\n",
    "        af_precision_list.append(precision)\n",
    "        af_recall_list.append(recall)\n",
    "    af_mean_precision = np.nanmean(np.array(af_precision_list))\n",
    "    af_mean_recall = np.nanmean(np.array(af_recall_list))\n",
    "    print( '\\nAF method: mean precision: ', af_mean_precision)\n",
    "    print ('AF method: mean recall: ', af_mean_recall)\n",
    "   \n",
    "    # For BF\n",
    "    bf_precision_list = []\n",
    "    bf_recall_list = []\n",
    "    print (\"\\n#########-------For BF-------#########\")\n",
    "    for grp in self.groups:\n",
    "        grp.generate_actual_recommendations(self.test_ratings, self.cfg.rating_threshold_bf)\n",
    "        (precision, recall, tp, fp) = grp.evaluate_bf()\n",
    "        bf_precision_list.append(precision)\n",
    "        bf_recall_list.append(recall)\n",
    "\n",
    "    bf_mean_precision = np.nanmean(np.array(bf_precision_list))\n",
    "    bf_mean_recall = np.nanmean(np.array(bf_recall_list))\n",
    "    print( '\\nBF method: mean precision: ', bf_mean_precision)\n",
    "    print ('BF method: mean recall: ', bf_mean_recall)\n",
    "\n",
    "    # For WBF\n",
    "    wbf_precision_list = []\n",
    "    wbf_recall_list = []\n",
    "    print (\"\\n#########-------For WBF-------#########\")\n",
    "    for grp in self.groups:\n",
    "        grp.generate_actual_recommendations(self.test_ratings, self.cfg.rating_threshold_wbf)\n",
    "        (precision, recall, tp, fp) = grp.evaluate_wbf()\n",
    "        wbf_precision_list.append(precision)\n",
    "        wbf_recall_list.append(recall)\n",
    "\n",
    "    wbf_mean_precision = np.nanmean(np.array(wbf_precision_list))\n",
    "    wbf_mean_recall = np.nanmean(np.array(wbf_recall_list))\n",
    "    print ('\\nWBF method: mean precision: ', wbf_mean_precision)\n",
    "    print ('WBF method: mean recall: ', wbf_mean_recall)\n",
    "   \n",
    "    \n",
    "    \n",
    "GroupRec.evaluation = evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are running all our proposed methods and evaluating them altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_methods(self, groups):\n",
    "    if (groups is None):\n",
    "        groups = self.groups\n",
    "    #PS: could call them without passing groups as we have already added groups to grouprec object\n",
    "    self.af_runner(groups, Aggregators.weighted_average)\n",
    "    self.bf_runner(groups, Aggregators.average_bf)\n",
    "    self.wbf_runner(groups, Aggregators.average_bf)\n",
    "\n",
    "    #evaluation\n",
    "    self.evaluation()\n",
    "GroupRec.run_all_methods = run_all_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Our class definitions end here***\n",
    "Starting from here, this can be treated as the main script for the entire code.\n",
    "\n",
    "First, Here we complete the matrix factorization with SGD method. The number of iterations is taken from the\n",
    "config file and MSE over the iterations is reported. We are only doing 3 iterations in this demo so the mse(error) is higher.\n",
    "For our results, we have done more iterations to get lesser mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from  ./data/u1.base ...\n",
      "Reading testing data from  ./data/u1.test ...\n",
      "3\n",
      "Doing matrix factorization...\n",
      "Iteration:  0\n",
      "training mse:  0.8027430160822857\n",
      "test mse:  1.0639167095348938\n",
      "Iteration:  1\n",
      "training mse:  0.7511717847111641\n",
      "test mse:  1.0054918713848724\n",
      "Iteration:  2\n",
      "training mse:  0.7194214654049347\n",
      "test mse:  0.9828198433288068\n"
     ]
    }
   ],
   "source": [
    "gr = GroupRec()\n",
    "print(gr.cfg.max_iterations_mf)\n",
    "gr.sgd_factorize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate small, medium and large groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Running for  small  groups *************\n",
      "generated groups (only first 5 are getting printed here): \n",
      "[81, 393, 730]\n",
      "[63, 282, 299]\n",
      "[95, 238, 468]\n",
      "[4, 259, 926]\n",
      "[22, 148, 482]\n",
      "\n",
      "******* Running for  medium  groups *************\n",
      "generated groups (only first 5 are getting printed here): \n",
      "[54, 112, 250, 320, 573]\n",
      "[342, 432, 514, 544, 818]\n",
      "[183, 202, 679, 698, 822]\n",
      "[103, 257, 307, 547, 557]\n",
      "[180, 182, 595, 728, 784]\n",
      "\n",
      "******* Running for  large  groups *************\n",
      "generated groups (only first 5 are getting printed here): \n",
      "[18, 40, 77, 166, 256, 332, 397, 798, 875, 928]\n",
      "[45, 78, 94, 162, 216, 267, 463, 527, 693, 898]\n",
      "[50, 151, 174, 234, 246, 376, 377, 613, 670, 881]\n",
      "[11, 73, 217, 245, 304, 352, 450, 533, 557, 873]\n",
      "[79, 215, 349, 374, 459, 667, 688, 723, 824, 919]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#generate groups programmatically\n",
    "#disjoint means none of the groups shares any common members     \n",
    "small_groups = Group.generate_groups(gr.cfg, gr.ratings, gr.test_ratings, gr.num_users, 10, gr.cfg.small_grp_size, disjoint=True)\n",
    "medium_groups = Group.generate_groups(gr.cfg, gr.ratings, gr.test_ratings, gr.num_users, 10, gr.cfg.medium_grp_size, disjoint=True)\n",
    "large_groups = Group.generate_groups(gr.cfg, gr.ratings, gr.test_ratings, gr.num_users, 10, gr.cfg.large_grp_size, disjoint=True)\n",
    "\n",
    "group_set = [small_groups, medium_groups, large_groups]\n",
    "group_type = ['small', 'medium', 'large']\n",
    "\n",
    "for idx, groups in enumerate(group_set):\n",
    "    if groups is []:\n",
    "        continue\n",
    "\n",
    "    # generated groups\n",
    "    n = 5\n",
    "    print ('\\n******* Running for ', group_type[idx], ' groups *************')\n",
    "    print ('generated groups (only first %d are getting printed here): ' % n)\n",
    "    for group in groups[:n]:\n",
    "        print(group.members)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******* Running for  small  groups *************\n",
      "\n",
      "#########-------For AF-------#########\n",
      "\n",
      "AF method: mean precision:  0.7622222222222221\n",
      "AF method: mean recall:  0.08131789768812825\n",
      "\n",
      "#########-------For BF-------#########\n",
      "\n",
      "BF method: mean precision:  1.0\n",
      "BF method: mean recall:  0.002585919252585919\n",
      "\n",
      "#########-------For WBF-------#########\n",
      "\n",
      "WBF method: mean precision:  1.0\n",
      "WBF method: mean recall:  0.002585919252585919\n",
      "\n",
      "******* Running for  medium  groups *************\n",
      "\n",
      "#########-------For AF-------#########\n",
      "\n",
      "AF method: mean precision:  0.837037037037037\n",
      "AF method: mean recall:  0.07562105119089552\n",
      "\n",
      "#########-------For BF-------#########\n",
      "\n",
      "BF method: mean precision:  0.791358024691358\n",
      "BF method: mean recall:  0.03190598074001741\n",
      "\n",
      "#########-------For WBF-------#########\n",
      "\n",
      "WBF method: mean precision:  0.7057142857142857\n",
      "WBF method: mean recall:  0.028513278853218083\n",
      "\n",
      "******* Running for  large  groups *************\n",
      "\n",
      "#########-------For AF-------#########\n",
      "\n",
      "AF method: mean precision:  0.8508192830561251\n",
      "AF method: mean recall:  0.06716446662261379\n",
      "\n",
      "#########-------For BF-------#########\n",
      "\n",
      "BF method: mean precision:  0.6608333333333334\n",
      "BF method: mean recall:  0.03507192339446012\n",
      "\n",
      "#########-------For WBF-------#########\n",
      "\n",
      "WBF method: mean precision:  0.7936026936026936\n",
      "WBF method: mean recall:  0.030270281924347487\n"
     ]
    }
   ],
   "source": [
    "# importing the required module \n",
    "import matplotlib.pyplot as plt\n",
    "for idx, groups in enumerate(group_set):\n",
    "    if groups is []:\n",
    "        continue\n",
    "    print ('\\n******* Running for ', group_type[idx], ' groups *************')\n",
    "\n",
    "    gr.add_groups(groups)\n",
    "    gr.run_all_methods(groups) \n",
    "    \n",
    "    gr.remove_groups(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
